<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Keegan Hines : github home page" />
    <link href="http://maxcdn.bootstrapcdn.com/font-awesome/4.2.0/css/font-awesome.min.css" rel="stylesheet">

    <link rel="stylesheet" type="text/css" media="screen" href="../stylesheets/stylesheet.css">
	<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-50962617-1', 'auto');
  ga('send', 'pageview');

</script>
<script type="text/javascript"
  src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>

    <title>Notes On Gaussian Processes</title>
  </head>

  <body>

    <!-- HEADER -->
<div id="header_wrap" class="outer">
        <header class="inner">
	  <ul class='nav' >
	   
          
	  <li><a href='mailto:keegan.hines@gmail.com' target='_blank'><i class="fa fa-envelope"></i></a></li>
	  <li><a href='https://twitter.com/keeghin' target='_blank'><i class="fa fa-twitter"></i></a></li>
	  <li><a href='https://www.linkedin.com/in/keeganhines' target='_blank'><i class="fa fa-linkedin"></i></a></li>
	  <li><a href='http://stackoverflow.com/users/4153882/keegan' target='_blank'><i class="fa fa-stack-overflow"></i></a></li>
	  <li><a href='https://github.com/keeganhines' target='_blank'><i class="fa fa-github"></i></a></li>
	  </ul>
	  <div >
          <h1 id="project_title" style='text-align:center;'>Keegan Hines</h1>
	  </div>

        
	<ul class="nav">
	<li ><a href="../index.html"> Home </a></li>
	 <li > <a href="../academics/index.html"> Academics </a></li>
	 <li ><a href="./index.html"> Blog </a> </li>
	</ul>
    </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h3>

<h1>Notes On Gaussian Processes</h1>

<p>
Back in grad school, I had a particular interest in a family of Nonparametric Bayesian methods involving the <a href="http://keeganhin.es/static/Biophys_2015_Hines_etal.pdf" target="_blank">Dirichlet Process</a>. Since that time, I have been meaning to catch up on the DP's close cousin, the Gaussian Process. I'm finally getting around to working through <a href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf" target="_blank"> Rasmussen and William's</a> book on this topic, so here I'm jotting down my notes. The code is kept in <a href="https://github.com/keeganhines/gp" target="blank"> this repo</a>.
</p>

<h4>Introduction</h4>

<p> While the DP is known to provide a probability distribution over the space of all probability distributions, the Gaussian Process can be thought of a probability distribution on the space of all smooth functions. That is, a single draw from a GP is a smooth function.  Importantly, we can use the GP as a prior for modeling any unknown non-linear relationships we want. Thus, if we have some finite number of observations relating X and Y, we can flexibly model the relationship between the two using a GP prior. Then, we can use the GP posterior for function estimation, interpolation, or forecasting. </p>

<p> Let's get a concrete example up front, then dive into some detail about the GP.  In the figure below, I've shown a small number of observations relating X and Y. </p>

<img src="static/gp/fake_data.png"> 

<p> Pardon the lack of axes labels. We can see that the observations span an x-range of about +/- 15 and the y-range spans a range of +/- 1. These data were generated from a particular function (y = sin(x)), but in practice we never know the true relationship between x and y. Instead, we wish to estimate this unknown non-inear relationship using the observations we do have. Further, it would be nice to not only have an estimate of the function y(x) but also some notion of how uncertain we are about that function. This is what Gaussian Process Regression gets us. To jump to the punchline right away, below I'm plotting the posterior distribution over y(x) when using a particular GP prior. </p>

<img src="static/gp/gp_obs_and_posterior.png"> 

<p>
The black crosses are the observations, the black line is the posterior mean, and the blue shaded region is a 95% credible interval. Notice that the width of the credible interval is non-constant. In regions where we have several observed data points, we're pretty well able to constrain our estimate of y(x). But in regions where we have very little observed data, we have little constraining power, and our uncertainy about our function grows. Again, the credible intervals give us a quantification of this uncertainty given our obervations: they show us what the set of possible functions might be that are consisent with our data. So let's dive into a little more detail about how GPs work.
</p>

<h4> Sampling a GP </h4>

<p> 
A Gaussian Process is characterized by two "parameters": a mean function and a covariance kernel. A single draw from a Gaussian Process is denoted as
</p>

\[
 g \sim GP(\mu, \mathscr{k})
\]

<p> where again g, being a draw from a GP, is a function over the domain of the kernel. As a concrete example, below are a few draws from a GP. I'll get to the details later on, but here we just want to have some visualizable examples of what's going on. </p>

<img src="static/gp/gp_samples.png">

<p>
The figure above shows exactly three draws from a particular GP. The domain of x is about minus 5 to plus 5, and we have a green function, a light-blue function, and a darker-blue function. Each of these functions is relatively smooth and slowly-varying, but they're each pretty different. They're each a random draw from the space of smooth functions. So in order to ease into understanding how to sample from GP priors and posteriors let's start with something more familiar.
 </p> 
<h6> Diversion about Gaussian Distributions </h6>

<p>
Something we're surely familiar with is drawing samples from a Gaussian distribution. 

\[
x \sim N(\mu, \sigma^2)
\]

Here, a single random variable x is drawn from a Normal distribution which is parameterized by a mean and a variance. We can visulize the full probability density of this distribution as well a few samples from it.
</p>
[insert figure]

<p>
While we were drawing univariate Gaussian random variables, we can generalize this to higher dimensions with the Multivariate Normal distribution, denoted as  


\[
\vec{x} \sim N(\vec{\mu}, \Sigma)
\]

Here, our random sample is now a vector is some higher-dimensional space. The distribution is still parameterized by a mean and a variance, but now their higher-dimensional cousins. There is a mean vector, which represents the centroid point in the higher-dimensional space. And then there is Sigma, the covariance matrix. This is an MxM matrix (where M is the dimensionality of the distribution) that quantifies the covariance of each of the dimensions of the space. A two-dimensional example makes things concrete.
</p>

<p>
\[
\vec{\mu} = (1,1)^T
\]

\[
\Sigma = \begin{bmatrix} 5 & 3 \\ 3  & 10\end{bmatrix}
\]

This covariance matrix quantifies how correlated each dimension is, and also the univariate marginal variance of each dimension. So the first row and first column denotes the covariance between dimension \(x_1\) and dimension \(x_1\), ie. how does \(x_1\) vary with itself, ie. what is \(Var(x_1)\). In the first row and second column, we have the element corresponding to the correlation betwee \(x_1\) and \(x_2\). Thus, with the Multivariate Normal distribution, we needn't assume that each dimension of our space is simply a draw from a univariate Normal which is completely independent of the other dimensions. Instead, we explicity model the correlation structure using \(\Sigma\).
</p>

<p>
Here's a few draws from the two-dimension Gaussian distribution above as well as the probability density contours of the distribution (note that they're elliptical).
</p>

[insert figure]

<p>
Again, by altering \(\Sigma\), we can change the shape of the distribution. Here's a second example with a different \(\Sigma\).
</p>

[insert figure]

<p>
The examples here have involved draws from two-dimensional Normal distributions, but all these premises apply for any higher dimensionality that we might pick. 
</p>

<h6> Back to GPs </h6>
<p>
When thinking about the domain of a function (let's call this domain \(x\)), we are typically considering an infinite number of points on the Real Line. Then, \(f(x)\) itself is also an infinite point set, We're going to start thinking about as an infininte dimensional vector. What if we could draw that vector randomly from probability distribution? Above, we were easily drawing two-dimensional vectors from a known probability distribution. With \(f(x)\), we will draw this infinite-dimensional vector from an infinite-dimensional stochastic process known as the Guassian Process. Thus, we are able to draw random functions from a probability distribution over the set of all funcitons. While this seems impossible, the clever thing about the GP (the reason it gets its name), is that while it is an inherently infinite dimensional object, all possible finite-dimensional marginal distributions are simply Multivariate Normal distributions with a simple mean vector and covariance matrix. 
</p>  

<p> The GP, being infinite dimensional, is not parameterized by a mean vector and covariance matrix. Instead, it is parameterized by a mean function, and a kernel function. 

\[
g \sim GP(\mu(x), \mathscr{k}(x,x'))
\]

<p>
The mean function \( \mu(x) \) is simply the average value of \(y(x)\) over all possible functions on the domain \(x\). As I mentioned, we can typically just set \( \mu(x)=0,  \forall x\).  
</p>

<p>
  The kernel function \( \mathscr{k}(x,x'):\mathbb{R} \times \mathbb{R} \mapsto \mathbb{R} \) is simply a function that takes two points from the domain, call them \(x\) and \(x'\), and computes some notion of "distance" between our expected functions at those two points. The kernel is used to quantify how similar we expect two points \(y\) to be for two points in the domain. There's lot of kernels to choose from and we'll meet just a few of them later on. As we'll see, they help us form a meaningful prior on what kinds of functions we expect to see. 
</p>

<p>
So here's an important thing to notice. The GP is parameterized by functions \( \mu \) and \(\mathscr{k}\) which are inherently infinite. But, for any finite set of points \(\{x_1, x_2, ..., x_N\}\) that we might care about, it is easy to compute things like \(\mu(x_1) \) and \( \mathscr{k}(x_1, x_2) \). Recall I said that any finite dimensional marginal of the GP is a Multivariate Normal. Therefore, the trick is that for any fininte point set we have, we can compute \( \vec{\mu} = \{\mu(x_1),...,\mu(x_N)\} \) and

\[
  \Sigma = \begin{bmatrix} \mathscr{k}(x_1,x_1)  & ... & \mathscr{k}(x_1,x_N) \\ 
  ...  & ... & ... \\ 
  \mathscr{k}(x_N,x_1)  & ... &  \mathscr{k}(x_N,x_N) \end{bmatrix}.
\]
</p>

<p>
So to draw a funciton from a GP prior, the procedure is actually quite simple. First, we pick a finite point set from our domain: this point set is where we care to evaluate our random function. We could pick an evenly-spaced grid, or a set of randomized points, absolutely doesn't matter. 
</p>

<pre> x = np.sort(np.random.uniform(-5,5,200)) </pre>

<p>
Next, let's pick a kernel and then calculate our \(200 \time 200 \) covariance matrix \( \Sigma \) but simply evaluating the kernel at all pairwise combinations of x values that we have. I'll discuss kernels more later, but for I'll use what's called the Square Exponential kernel. This simple kernel quantifies our prior belief that \(y(x)\) should be similar for any two values of \(x\) that are close, and should be less correlated the father apart the two values of \(x\) are. The result is that our random functions should be kind of smooth and evolve rather slowly. So we'll use this kernel to calculate our finite-sized \(\Sigma \).
</p>

<pre> 
kernel = SquaredExponential(1,1)

Sigma = np.zeros((len(x), len(x)))
for i in range(0, len(x)):
  for j in range(0, len(x)):
    Sigma[i,j] = kernel(x[i], x[j])
</pre>

<p> Now we have a covariance matrix for a regular old Multivariate Normal distribution. For our mean vector, let's just do a vector of zeros. So now, drawing a sample from a GP prior is as simple as, </p>

<pre> g = multivariate_normal(np.zeros(len(x)), Sigma) </pre>

<p>
That's it. Now we have a function \(g(x) \) that we can visualize against \(x\). Again, here's three different draws from that Multivariate Normal distribution.
</p>

<img src="static/gp/gp_samples.png">

<p>
They each evole rather slowly. The mean (over all samples) is approximately 0 at all \(x\). This behavior is indicative of using the Squared Exponential kernel for our prior. Before we get to posterior inference, let's dive into some more detail about kernel choices.
</p>  


<h4> Kernels </h4>
<p>
There's many kinds of kernels to choose from and our choice of kernel (and their hyperparameters) has a big impact on the properties of functions that result from our GP. There's a great <a href="http://www.cs.toronto.edu/~duvenaud/cookbook/index.html" target="_blank"> kernel cookbook</a> that you can use for reference, and I will stick to their terminology and definitions. So let's first dive more into that Squared Exponential kernel.
</p>

<h6> Squared Exponential</h6>

The SE kernel is perhaps the simplest and most common for tasks such as function estimation and interpolation. The functional form should look pretty familiar.

\[
\mathscr{k}(x,x') = \sigma^2 \textrm{exp}\left(- \frac{(x - x')^2}{2 \mathscr{l}^2}\right)
\]

<h6> Periodic</h6>

<h6> Locally Periodic</h6>

<h4> Posterior Estimation </h4>


      </section>
    </div>


    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p>Thanks for taking  a look</p>
      </footer>
    </div>

    

  </body>
</html>
